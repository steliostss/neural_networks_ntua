{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification ΙΙ.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"id":"8zR-sydoN4o5"},"source":["# Παραμετρικοί και μη-παραμετρικοί ταξινομητές, bias - variance trade-off\n","O Gaussian Naive Bayes είναι ένας παραμετρικός ταξινομητής. Οι παραμετρικοί ταξινομητές κάνουν κάποια υπόθεση για την κατανομή (των χαρακτηριστικών) των δεδομένων και την προσδιορίζουν μέσω παραμέτρων. Στην περίπτωση του Gaussian Naive Bayes, η υπόθεση είναι η κανονική κατανομή και οι παράμετροι είναι τα $μ$ και $σ^2$ των χαρακτηριστικών. Αντίθετα, οι μη-παραμετρικές μέθοδοι δεν κάνουν καμία υπόθεση για την κατανομή των δεδομένων. Προσοχή: και οι μη-παραμετρικοί ταξινομητές έχουν παραμέτρους (και πάρα πολλές σε ορισμένες περιπτώσεις, τα βάρη των νευρωνικών για παράδειγμα) που επηρ\n","εάζουν τη λειτουργία τους αλλά δεν σχετίζονται με κάποια υπόθεση κατανομής για τα δεδομένα. \n","\n","Σε γενικές γραμμές οι παραμετρικοί ταξινομητές είναι απλούστεροι, ταχύτεροι στις φάσεις train/test και χρειάζονται λιγότερα δεδομένα εκπαίδευσης. Από την άλλη, έχουν γενικά μικρότερη χωρητικότητα (capacity), δηλαδή μπορούν να διαχωρίσουν τις κλάσεις σε προβλήματα σχετικά μικρότερων διαστάσεων ενώ η απαίτηση τα πραγματικά δεδομένα να ακολουθούν μια ακριβή κατανομή είναι πολύ ισχυρή και δεν επαληθεύεται πρακτικά. Αντιστρόφως, οι μη παραμετρικοί ταξινομητές είναι πιο αργοί στην εκπαίδευση, έχουν γενικά μεγαλύτερες απαιτήσεις χώρου/μνήμης και χρειάζονται περισσότερα δεδομένα αλλά έχουν μεγαλύτερη χωρητικότητα, μπορούν να μάθουν δυσκολότερα προβλήματα και να έχουν καλύτερη απόδοση σε μεγαλύτερα datasets. \n","\n","Οι μη παραμετρικοί ταξινομητές μπορούν να εμφανίσουν επίσης εντονότερα το πρόβλημα της υπερεκπαίδευσης (overfitting), δηλαδή να προσαρμοστούν υπερβολικά στα δεδομένα εκπαίδευσης και να μειωθεί η ικανότητα γενίκευσης (generalisation) τους σε νέα δείγματα. Γενικά στη φάση της εκπαίδευσης προσπαθούμε να επιτύχουμε μια καλή ισορροπία μεταξυ της απόκλισης (bias) και της διακύμανσης (variance) από τις πραγματικές τιμές.\n","\n","![Bias-Variance trade off](http://www.bogotobogo.com/python/scikit-learn/images/NeuralNetwork7-Overfitting/Overfitting.png \"Bias-Variance trade off\")\n","\n","\n","Ένα παράδειγμα μη-παραμετρικού ταξινομητή που θα εξετάσουμε είναι ο kNN (k-Nearest-Neighbours). "]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"4ZdyhJaEN4o8"},"source":["# k Nearest Neighbors Classifier (kNN)\n","O kNN είναι ένας μη παραμετρικός ταξινομητής βασισμένος σε παραδείγματα (instance-based). Η αρχή λειτουργίας του είναι πολύ απλή. Για ένα νέο δείγμα προς ταξινόμηση, πρώτα υπολογίζουμε τους k πλησιέστερους γείτονές του (στον ν-διάστατο χώρο των χαρακτηριστικών εισόδου) με βάση κάποια συνάρτηση απόστασης, συνήθως ευκλείδεια\n","$$d(x, x') = \\sqrt{\\left(x_1 - x'_1 \\right)^2 + \\left(x_2 - x'_2 \\right)^2 + \\dotsc + \\left(x_n - x'_n \\right)^2}$$\n","Η κλάση του νέου δείγματος θα είναι η κλάση της πλειοψηφίας των k γειτόνων (διαλέγουμε k περιττό γενικά), είτε απλά υπολογισμένη (άθροισμα) είτε (αντίστροφα) ζυγισμένη με βάση την απόσταση του κάθε γείτονα. \n","\n","Ο kNN δεν έχει πρακτικά φάση εκπαίδευσης. Ωστόσο, για να ταξινομήσουμε ένα νέο δείγμα στην φάση test,  πρέπει να συγκρίνουμε την απόστασή του με κάθε δείγμα του train set. Αυτό σημαίνει ότι για την ταξινόμηση είναι απαραίτητα όλα τα δείγματα εκπαίδευσης (εξού και η ονομασία \"instance-based\", ενώ στον Naive Bayes χρειαζόμαστε μόνο τις παραμέτρους $μ$ και $σ^2$). Αυτό σημαίνει ότι ο kNN είναι πιο απαιτητικός και σε χώρο (αποθήκευση όλων των δειγμάτων) και σε χρόνο (υπολογισμός όλων των αποστάσεων για κάθε νέο δείγμα).\n","# Υπερπαράμετρος k\n","Το k της γειτονιάς του kNN είναι μια υπερπαράμετρος του ταξινομητή. Μια άλλη υπερπαράμετρος για παράδειγμα είναι η συνάρτηση της απόστασης. Οι υπερπαράμετροι είναι επιλογές που γίνονται από τον σχεδιαστή του συστήματος και δεν μπορούμε να ξέρουμε τις βέλτιστες τιμές τους αν πρώτα δεν τις αξιολογήσουμε εμπειρικά σε δεδομένα.  Ένα άλλο παράδειγμα υπερπαραμέτρου είναι ο αριθμός των κρυφών νευρώνων σε ένα MLP. Στην περίπτωση του kNN το k ελέγχει το trade-off μεταξύ μεταξύ απόκλισης και διακύμνανσης.\n","\n","Έαν θέσουμε μικρό k, πχ k=1 παίρνουμε ένα ταξινομητή με υψηλή διακύμανση και χαμηλή απόκληση. Ο ταξινομητής τείνει να αγνοεί τη συνολική κατανομή και αποφασίζει μόνο από το κοντινότερο δείγμα. Στην περίπτωση k=1 το σύνορο απόφασης (decision boundary) περνά από τις μεσοκάθετους γειτονικών δειγμάτων διαφορετικής κλάσης. \n","\n","![kNN k=1](https://i.stack.imgur.com/UG81y.png \"kNN with k=1\")\n","\n","Αν διαλέξουμε μεγαλύτερο k, φτιάχνουμε ένα ταξινομητή με χαμηλότερη διακύμανση και υψηλότερη απόκλιση. Θα ταξινομίσει λάθος περισσότερα αποκλίνοντα δείγματα (outliers) αλλά θα σέβεται περισσότερο τη συνολική κατανομή.\n","![kNN k=20](https://i.stack.imgur.com/FZITG.png \"kNN with k=20\")\n","\n","# Το Iris dataset\n","Για να μελετήσουμε τον kNN, θα φορτώσουμε το dataset [\"Iris\"](https://archive.ics.uci.edu/ml/datasets/Iris). Το Iris είναι το διασημότερο dataset στο Machine Learning. Το χρησιμοποίησε πρώτος το 1936 ο διάσημος στατιστικός [Ronald Fisher](https://en.wikipedia.org/wiki/Ronald_Fisher) και αποτελείται από 50 παρατηρήσεις για κάθε είδος του άνθους Iris (Iris setosa, Iris virginica και Iris versicolor). Για κάθε δείγμα μετρήθηκαν 4 χαρακτηριστικά: το πλάτος και το μήκος των πετάλων και των σεπάλων.\n","\n","![Iris petals & sepals](https://www.integratedots.com/wp-content/uploads/2019/06/iris_petal-sepal-e1560211020463.png \"Iris petals & sepals\")\n","\n","Σημειώστε ότι με το Iris και τρεις κατηγορίες και άρα έχουμε multiclass και όχι binary classification. Εισάγουμε το Iris στο notebook."]},{"cell_type":"code","metadata":{"id":"z1uirGVkN4o-"},"source":["# Load Iris and organize our data\n","from sklearn.datasets import load_iris\n","data = load_iris()\n","label_names = data['target_names']\n","labels = data['target']\n","feature_names = data['feature_names']\n","features = data['data']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSoLp7_UN4pB"},"source":["# Ας τυπώσουμε τα ονόματα των χαρκτηριστικών\n","print(feature_names)\n","print(label_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HIZ_g3-MN4pG"},"source":["# Χρησιμοποιούμε τη γνωστή train_test_split για να διαχωρίσουμε σε train και test set\n","# το (int) όρισμα \"random_state\" είναι το seed της γεννήτριας τυχαίων αριθμών\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.40, random_state=78)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m0D806YqN4pJ"},"source":["Διαλέγουμε τυχαία τιμή 5 για την υπερπαράμετρο k, εκπαιδεύουμε ένα kNN classifier και υπολογίζουμε την πιστότητα του στο Iris, στο προηγούμενο train/test split"]},{"cell_type":"code","metadata":{"id":"X1vVA6_zN4pK"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","knn = KNeighborsClassifier(n_neighbors=1)\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","print(accuracy_score(y_test, pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zc4Mx50nN4pO"},"source":["# Ρύθμιση υπερπαραμέτρων με διασταυρούμενη επικύρωση (Cross Validation)\n","Ένας προφανής τρόπος να βρούμε τη βέλτιστη τιμή του k, να πραγματοποιήσουμε δηλαδή επικύρωση του μοντέλου, είναι ο ακόλουθος. Για k=1 μέχρι k=κάποιο n, κάνουμε fit τον ταξινομητή στο train set και μετράμε την απόδοση στο test set. Ο ταξινομητής με k που δίνει το μικρότερο σφάλμα ταξιμόνόμησης σύμφωνα με κάποιο κριτήριο (εδώ η πιστότητα) στο test set θα είναι ο βέλτιστος. Όμως, αν ακολουθήσουμε αυτή τη στρατηγική, ουσιαστικά κάνουμε υπερεκπαίδευση, καθώς χρησιμοποιούμε το test set ως training set, δηλαδή βελτιστοποιούμε κάποιο κριτήριο σφάλματος πάνω στο test set. Αυτό μπορεί να είναι επιβλαβές για την ικανότητα γενίκευσης του ταξινομητή: το test set χρησιμεύει μόνο για την τελική εκτίμηση της απόδοσης του ταξινομητή.\n","\n","Για να ακολουθήσουμε σωστά το πρωτόκολλο, αυτό που πρέπει να κάνουμε είναι να χρησιμοποιήσουμε μόνο το πραγματικό training set για να διαλέξουμε τις βέλτιστες υπερπαραμέτρους. Θα μπορούσαμε να κρατήσουμε ένα ποσοστό δειγμάτων ως σύνολο επικύρωσης (validation set πχ άλλο ένα 1/3) του training set και να ακολουθήσουμε την προηγούμενη διαδικασία: εκπαίδευση στο 1/3 training set, επικύρωση σε 1/3 και τελικά αξιολόγηση στο 1/3 data set. Ωστόσο αυτή η μεθοδολογία \"αχρηστεύει\" μεγάλο μέρος του dataset (τα 2/3) ως προς την εκπαίδευση του ταξινομητή. Πρακτικά λοιπόν, προτιμούμε να χρησιμοποιούμε τη μέθοδο της διασταυρούμενης επικύρωσης (Cross Validation).\n","\n","Στο Cross Validation αρχικά χωρίζουμε το training set σε έναν αριθμό \"πτυχών\" (folds). Συνηθισμένες τιμές είναι το 5 και το 10 (5-fold και 10-fold CV). Στη συνέχεια, για κάθε k-fold (άσχετο από το k του kNN), θεωρούμε ότι τα k μείον 1 folds είναι training set και ότι το fold που αφήσαμε έξω είναι το test set. Υπολογίζουμε τη μετρική σφάλματός μας στο test set που ορίζει το fold. Επαναλαμβάνουμε τη διαδικασία για τα k folds για κάθε τιμή των υπερπαραμέτρων και υπολογίζουμε τη μέση τιμή της μετρικής του σφάλματος. Με αυτό τον τρόπο, αφενός είμαστε αμερόληπτοι στην αξιολόγηση αφήνοντας τελείως έξω το test set και αφετέρου χρησιμοποιούμε αποτελεσματικά τα δεδομένα εκπαίδευσης: τα χρησιμοποιούμε όλα και παίρνοντας τη μέση τιμή εξαλείφουμε πιθανές ανωμαλίες στα δεδομένα.\n","\n","![Cross validation](https://sebastianraschka.com/images/faq/evaluate-a-model/k-fold.png \"Cross Validation\")\n","\n","To Scikit Learn έχει συναρτήσεις για να κάνει αυτόματα cross validation (να ορίζει folds και να  υπολογίζει τιμές και μέσους όρους). Θα κάνουμε 5 fold cross validation για να υπολογίσουμε το βέλτιστο k του kNN στο Iris."]},{"cell_type":"code","metadata":{"id":"3q1krR5RN4pO"},"source":["from sklearn.model_selection import cross_val_score\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# φτιάχνουμε μια λίστα από το 1 έως το 50\n","myList = list(range(1,50))\n","# Κρατάμε μόνο τα περιττά k\n","neighbors = list(filter(lambda x: x % 2 != 0, myList))\n","# empty list that will hold cv scores\n","cv_scores = []\n","# perform 5-fold cross validation\n","for k in neighbors:\n","    knn = KNeighborsClassifier(n_neighbors=k)\n","    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy')\n","    cv_scores.append(scores.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Usr5iw5N4pR"},"source":["Θα τυπώσουμε την τιμή του σφάλματος σε γραφική παράσταση ως συνάρτηση του k για το training set, και την τιμή της πιστότητας στο test set για το βέλτιστο k (υπολογισμένο στο training set)"]},{"cell_type":"code","metadata":{"id":"U7izRFzIPZHU"},"source":["# Κάνουμε update την matplotlib, ίσως χρειαστεί restart του runtime\n","!pip install --upgrade matplotlib"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TiB323l-N4pS"},"source":["# Κάνουμε import την matplotplib\n","import matplotlib.pyplot as plt\n","\n","# το σφάλμα είναι το αντίστροφο της πιστότητας\n","mean_error = [1 - x for x in cv_scores]\n","\n","# plot misclassification error vs k\n","plt.plot(neighbors, mean_error)\n","plt.xlabel('Number of Neighbors K')\n","plt.ylabel('Misclassification Error')\n","plt.show()\n","\n","# determining best k\n","optimal_k = neighbors[mean_error.index(min(mean_error))]\n","print(\"The optimal number of neighbors (calculated in the training set) is %d\" % optimal_k)\n","\n","# για το optimal k παίρνουμε και τα αποτέλεσματα στο test set\n","knn = KNeighborsClassifier(n_neighbors = optimal_k)\n","knn.fit(X_train, y_train)\n","pred = knn.predict(X_test)\n","print(\"\\nOptimal accuracy on the test set is\", accuracy_score(y_test, pred), \"with k=\", optimal_k)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E_vqjGz4HYFm"},"source":["Δεν είναι απαραίτητο ότι η βέτιστη τιμή της υπερπαραμέτρου που βρίσκουμε θα έχει τη βέλτιστη απόδοση στο test set, αλλά είμαστε σίγουροι ότι θα είναι μια πολύ καλή τιμή - κοντά στη βέλτιστη."]}]}