{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Shakespeare.ipynb","provenance":[],"authorship_tag":"ABX9TyM/rzwPXCW+rpYjmN0y1+qE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"C7j0nuRg8_q5"},"source":["import tensorflow as tf\r\n","\r\n","import numpy as np\r\n","import os\r\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eHVqo7ahIIe7"},"source":["tf.config.list_physical_devices(\r\n","    device_type=None\r\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7smv0JzO9TXX"},"source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZkemE62M9Yqg"},"source":["# Read, then decode for py2 compat.\r\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n","# length of text is the number of characters in it\r\n","print('Length of text: {} characters'.format(len(text)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rBwvCWQ99fjn"},"source":["# Take a look at the first 250 characters in text\r\n","print(text[:250])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"myv1yKMd9iwe"},"source":["# The unique characters in the file\r\n","vocab = sorted(set(text))\r\n","print('{} unique characters'.format(len(vocab)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yCMDZM3_9l6l"},"source":["# Creating a mapping from unique characters to indices\r\n","char2idx = {u:i for i, u in enumerate(vocab)}\r\n","idx2char = np.array(vocab)\r\n","\r\n","text_as_int = np.array([char2idx[c] for c in text])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MyMTCHE9ojW"},"source":["print('{')\r\n","for char,_ in zip(char2idx, range(20)):\r\n","    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\r\n","print('  ...\\n}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"glWBaDRz9rMt"},"source":["# Show how the first 13 characters from the text are mapped to integers\r\n","print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1NFGRYT9uoo"},"source":["# The maximum length sentence you want for a single input in characters\r\n","seq_length = 100\r\n","examples_per_epoch = len(text)//(seq_length+1)\r\n","\r\n","# Create training examples / targets\r\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\r\n","\r\n","for i in char_dataset.take(5):\r\n","    print(idx2char[i.numpy()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d_WmFsAX924v"},"source":["sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\r\n","\r\n","for item in sequences.take(5):\r\n","    print(repr(''.join(idx2char[item.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AQ8MDE7X96GT"},"source":["def split_input_target(chunk):\r\n","    input_text = chunk[:-1]\r\n","    target_text = chunk[1:]\r\n","    return input_text, target_text\r\n","\r\n","dataset = sequences.map(split_input_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tjX1WMOf9-tW"},"source":["for input_example, target_example in  dataset.take(1):\r\n","    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\r\n","    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fg_SXNqx-A99"},"source":["for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\r\n","    print(\"Step {:4d}\".format(i))\r\n","    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\r\n","    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGSsoK1N-GxI"},"source":["# Batch size\r\n","BATCH_SIZE = 64\r\n","\r\n","# Buffer size to shuffle the dataset\r\n","# (TF data is designed to work with possibly infinite sequences,\r\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\r\n","# it maintains a buffer in which it shuffles elements).\r\n","BUFFER_SIZE = 10000\r\n","\r\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n","\r\n","dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFrmneTS-MWj"},"source":["# Length of the vocabulary in chars\r\n","vocab_size = len(vocab)\r\n","\r\n","# The embedding dimension\r\n","embedding_dim = 256\r\n","\r\n","# Number of RNN units\r\n","rnn_units = 1024"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z77OEFZfdwLl"},"source":["![](https://www.tensorflow.org/tutorials/text/images/text_generation_training.png)"]},{"cell_type":"code","metadata":{"id":"a49ibMwb-PxN"},"source":["def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\r\n","    model = tf.keras.Sequential([\r\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim,\r\n","                                  batch_input_shape=[batch_size, None]),\r\n","        tf.keras.layers.GRU(rnn_units,\r\n","                            return_sequences=True,\r\n","                            stateful=True,\r\n","                            recurrent_initializer='glorot_uniform'),\r\n","        tf.keras.layers.Dense(vocab_size)\r\n","    ])\r\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEruK7xQ-SNM"},"source":["model = build_model(\r\n","    vocab_size=len(vocab),\r\n","    embedding_dim=embedding_dim,\r\n","    rnn_units=rnn_units,\r\n","    batch_size=BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w1fBdnQi-UOb"},"source":["for input_example_batch, target_example_batch in dataset.take(1):\r\n","    example_batch_predictions = model(input_example_batch)\r\n","    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GozaajUf-YdL"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8n96TLL5-bUh"},"source":["sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\r\n","sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Li_HTHLP-mln"},"source":["sampled_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ot86aJCI-oTW"},"source":["print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\r\n","print()\r\n","print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HoHDm0Ck-wR0"},"source":["def loss(labels, logits):\r\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\r\n","\r\n","example_batch_loss = loss(target_example_batch, example_batch_predictions)\r\n","print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\r\n","print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrWTenWG-5nE"},"source":["model.compile(optimizer='adam', loss=loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3B64wYl-7q5"},"source":["# Directory where the checkpoints will be saved\r\n","checkpoint_dir = './training_checkpoints'\r\n","# Name of the checkpoint files\r\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\r\n","\r\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\r\n","    filepath=checkpoint_prefix,\r\n","    save_weights_only=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-DO7qf2G-9y4"},"source":["EPOCHS = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rekQFLRT_Adq"},"source":["history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qdGN9Q0AAH52"},"source":["tf.train.latest_checkpoint(checkpoint_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-MWsG08AK2-"},"source":["model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\r\n","\r\n","model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\r\n","\r\n","model.build(tf.TensorShape([1, None]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dWscEq_AObR"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SO5PsLdt_Sfn"},"source":["def generate_text(model, start_string):\r\n","    # Evaluation step (generating text using the learned model)\r\n","\r\n","    # Number of characters to generate\r\n","    num_generate = 1000\r\n","\r\n","    # Converting our start string to numbers (vectorizing)\r\n","    input_eval = [char2idx[s] for s in start_string]\r\n","    input_eval = tf.expand_dims(input_eval, 0)\r\n","\r\n","    # Empty string to store our results\r\n","    text_generated = []\r\n","\r\n","    # Low temperature results in more predictable text.\r\n","    # Higher temperature results in more surprising text.\r\n","    # Experiment to find the best setting.\r\n","    temperature = 1.0\r\n","\r\n","    # Here batch size == 1\r\n","    model.reset_states()\r\n","    for i in range(num_generate):\r\n","        predictions = model(input_eval)\r\n","        # remove the batch dimension\r\n","        predictions = tf.squeeze(predictions, 0)\r\n","\r\n","        # using a categorical distribution to predict the character returned by the model\r\n","        predictions = predictions / temperature\r\n","        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\r\n","\r\n","        # Pass the predicted character as the next input to the model\r\n","        # along with the previous hidden state\r\n","        input_eval = tf.expand_dims([predicted_id], 0)\r\n","\r\n","        text_generated.append(idx2char[predicted_id])\r\n","\r\n","    return (start_string + ''.join(text_generated))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3v_L0bw8_9DR"},"source":["print(generate_text(model, start_string=u\"ROMEO: \"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ru9bVGIIzbs1"},"source":["# 10 EPOCHS\n","\n","ROMEO: Boapon every or te\n","But it comest against this, bold complites: yet do I hunt yet touchmone.\n","\n","BUCKINGHAM:\n","But in these forbiard man is only shall, upon him.\n","The best to him not alter than a good\n","To pluck a tear.\n","\n","ROMEO:\n","Though prock him that like go with me and has his\n","mother is here.\n","\n","RDOLTES:\n","Meth you indole.\n","\n","KING HENRY VI:\n","By burg, my good lord, wilt hold your hand.\n","\n","KING EDWARD IV:\n","Bleeding another! But what\n","Is deep with your love, and but of\n","his cau know will be your mistress.\n","\n","CALIBAN:\n","His face,\n","By hee shall be leading shame in all these farewell. Will't we bid\n","should pluch his outuke him, or hast thou lives to wash you thou didst beng him not\n","Without make worse it, and\n","will'd again.\n","Come, my woem they can debised,\n","That leptun reason ususurate mine\n","enter's vow, and want the fieed agends\n","To keep such windman in light not his state,\n","No storan heaviness or griefs, so says it is a preserver,\n","Spe knowing the poblow friendsame news.\n","\n","Second Servingman:\n","Now, by my crown III:\n","Your and yo"]},{"cell_type":"markdown","metadata":{"id":"Zc_Hvpbr01QL"},"source":["# 20 EPOCHS\n","\n","ROMEO: this good gentleman of all the oath,\n","O'erilance of your honour, O, thus: 'tis in rage\n","Give me leave to put a day longer,\n","I am usurp and wife he wislives to gife;\n","We see the prince and beartily I have to return?\n","O, he's appointments and reverend suspicion\n","Than what we have thee part from tress,\n","Which was to wear summed without loveth thus:\n","'This but begin require:\n","But Romeo, these peers are like an executioner,\n","I would say the breath help to age committed by:\n","As't brought them of:\n","Hold, that I can be curred before, I cuse thy life;\n","There are the vervity of his preservative,\n","To other down and afflicts my cheeks.\n","\n","KING RICHARD III:\n","Be not so quick, sir.\n","\n","LUCIO:\n","How now! a spring,\n","fairly Richmond, in those which we would art,\n","And each at himation.\n","Where is My say'st thou did, this shop are not admired;\n","We page\n","As little power at the Forces redeem these VIRGS:\n","Give me thy humble sweet sold.\n","\n","MAMILLIUS:\n","Not in the world, that were my wounds. Yet, go you inwort forty; but that I mourn with th"]}]}