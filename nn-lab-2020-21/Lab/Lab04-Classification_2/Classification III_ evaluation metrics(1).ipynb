{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Classification III: evaluation metrics.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"6emEWnHI-78w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610724964331,"user_tz":-120,"elapsed":29865,"user":{"displayName":"Chris Chiletzaris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYqXVJ56Yk9AGghEtEYviJe1INnpbbxzJ9yAYX=s64","userId":"08886392243558337108"}},"outputId":"c4a02eee-4e0e-40a5-c7df-e739f7174916"},"source":[" # κάνουμε update τις βιβλιοθήκες μας\n","!pip install -U scikit-learn\n","!pip install --upgrade pandas\n","!pip install -U scipy\n","\n","!pip install --upgrade numpy "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting scikit-learn\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/aa/db462d385c56905b731403885454188683f63c86ea68900f6f7e7558b5fa/scikit_learn-0.24.0-cp36-cp36m-manylinux2010_x86_64.whl (22.2MB)\n","\u001b[K     |████████████████████████████████| 22.2MB 5.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.0.0)\n","Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.19.5)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n","Installing collected packages: threadpoolctl, scikit-learn\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","Successfully installed scikit-learn-0.24.0 threadpoolctl-2.1.0\n","Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Collecting scipy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/89/63171228d5ced148f5ced50305c89e8576ffc695a90b58fe5bb602b910c2/scipy-1.5.4-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n","\u001b[K     |████████████████████████████████| 25.9MB 65.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.5)\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: scipy\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","Successfully installed scipy-1.5.4\n","Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NO_qd5W7N4pX"},"source":["# Αξιολόγηση ταξινόμησης με ακρίβεια, ανάκληση και F1, μέσοι όροι\n","Η πιστότητα είναι μια πολύ χρήσιμη και πρακτική μετρική της απόδοσης ενός ταξινομητή. Ωστόσο, δεν αρκεί για μια ολοκληρωμένη μελέτη της απόδοσής του. Είναι τυπικό για όλα τα προβλήματα machine learning να χρησιμοποιούμε διαφορετικές μετρικές για να μελετήσουμε τις ιδιότητες των δεδομένων. \n","\n","Καταρχάς, ας ξαναφέρουμε το Iris dataset και τον kNN και ας κάνουμε προβλέψεις με τον default kNN (είναι ακριβώς ο κώδικας από το προηγούμενο notebook σε ένα cell)"]},{"cell_type":"code","metadata":{"id":"M7S5oMasoiux","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610724964944,"user_tz":-120,"elapsed":30436,"user":{"displayName":"Chris Chiletzaris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYqXVJ56Yk9AGghEtEYviJe1INnpbbxzJ9yAYX=s64","userId":"08886392243558337108"}},"outputId":"526704f5-c753-430e-d8cd-305934a9ed3a"},"source":["# Load Iris and organize our data\n","from sklearn.datasets import load_iris\n","from sklearn.metrics import confusion_matrix\n","data = load_iris()\n","label_names = data['target_names']\n","labels = data['target']\n","feature_names = data['feature_names']\n","features = data['data']\n","# Χρησιμοποιούμε τη γνωστή train_test_split για να διαχωρίσουμε σε train και test set\n","# το (int) όρισμα \"random_state\" είναι το seed της γεννήτριας τυχαίων αριθμών (αν του δώσουμε τιμή θα παράξει την ίδια σειρά τυχαίων αριθμών)\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.40, random_state=78)\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","for i in range(1,20):\n","  knn = KNeighborsClassifier(n_neighbors=i)\n","  knn.fit(X_train, y_train)\n","  pred = knn.predict(X_test)\n","  # Compute confusion matrix\n","  cnf_matrix = confusion_matrix(y_test, pred)\n","  # τυπώνουμε τα labels\n","  print(label_names, \"\\n\")\n","  # τυπώνουμε το confusion matrix\n","  print(cnf_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 22  1]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 22  1]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 22  1]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 22  1]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 22  1]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n","['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Swg5-nF2pbsx"},"source":["Η βάση για τις μετρικές απόδοσης των ταξινομητών είναι ο πίνακας σύγχυσης (confusion matrix). O πίνακας σύχγυσης $C$  είναι τέτοιος ώστε το $C_{i, j}$ είναι ίσο με τα δείγματα που ενώ ανήκουν στην κατηγορία $i$ ταξινομήθηκαν στην κατηγορία  $j$. Για το Iris, στο συγκεκριμένο train/test split, o πίνακας σύγχυσης είναι:"]},{"cell_type":"code","metadata":{"id":"HPz7jmqolTDM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mcMHwGmJN4pZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610724964950,"user_tz":-120,"elapsed":30427,"user":{"displayName":"Chris Chiletzaris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYqXVJ56Yk9AGghEtEYviJe1INnpbbxzJ9yAYX=s64","userId":"08886392243558337108"}},"outputId":"5a5931e2-b0a0-4447-8172-952e2340e41b"},"source":["from sklearn.metrics import confusion_matrix\n","# Compute confusion matrix\n","cnf_matrix = confusion_matrix(y_test, pred)\n","# τυπώνουμε τα labels\n","print(label_names, \"\\n\")\n","# τυπώνουμε το confusion matrix\n","print(cnf_matrix)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['setosa' 'versicolor' 'virginica'] \n","\n","[[20  0  0]\n"," [ 0 21  2]\n"," [ 0  0 17]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UUP5FyOdN4pe"},"source":["που σημαίνει ότι 1 δείγμα που άνηκε κανονικά στο είδος _I.versicolor_ ταξινομήθηκε λανθασμένα στο είδος _I.virginica_. Τα στοιχεία της διαγωνίου είναι αληθινά θετικά δείγματα (true positive) της κάθε κλάσης. Για κάθε κλάση $i$ τα στοιχεία της γραμμής $i$ εκτός της διαγωνίου είναι λάνθασμένα αρνητικά δείγματα (false negative) της κλάσης και τα στοιχεία της κολώνας $i$ εκτός της διαγωνίου είναι λανθασμένα θετικά δείγματα (false positive) της κλάσης. \n","\n","\n","<figure>\n","  <center>\n","  <img src=\"http://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png\" alt=\"Confusion Matrix\">\n","   <figcaption>Confusion matrix στο Iris - αλλος ταξινομητής (SVM)</figcaption>\n","  </center>\n","</figure>\n","\n","\n","Σε περίπτωση δυαδικού ταξινομητή $C_{0,0}$ είναι τα αληθινά αρνητικά δείγματα (true negative, η κλάση 0 θεωρείται η αρνητική), $C_{1,0}$ είναι τα λανθασμένα αρνητικά δείγματα, $C_{1,1}$ τα αληθινά θετικά δείγματα και $C_{0,1}$ τα λανθασμένα θετικά. Για παράδειγμα:"]},{"cell_type":"code","metadata":{"id":"knmN5cfFN4pf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610724964957,"user_tz":-120,"elapsed":30409,"user":{"displayName":"Chris Chiletzaris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYqXVJ56Yk9AGghEtEYviJe1INnpbbxzJ9yAYX=s64","userId":"08886392243558337108"}},"outputId":"0c7bc37a-6767-4722-e3a9-54541b93290a"},"source":["# παράδειγμα confusion matrix σε δυαδική ταξινόμηση\n","tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n","print(tn, fp, fn, tp)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0 2 1 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"L5YF7Z4BN4pi"},"source":["Συχνά στην δυαδική ταξινόμηση θεωρούμε θετική την πιο σπάνια κλάση ή το φαινόμενο προς εντοπισμό (πχ διαβητικός). \n","\n","![CMMulti2](https://drive.google.com/file/d/17t5de5tpSYeIwufkHilTPBxCCNo6F2C0/view?usp=sharing)\n","\n","\n","Ορίζουμε:\n","Ακρίβεια -Precision- ($P$) είναι ο λόγος των true positives ($T_p$) ως προς τον αριθμό των true positives συν τον αριθμό των false positives ($F_p$).\n","$$P = \\frac{T_p}{T_p+F_p}$$\n","Ανάκληση -Recall- ($R$) είναι ο λόγος των true positives ($T_p$) ως προς τον αριθμό των true positives συν τον αριθμό των false negatives ($F_n$).\n","$$R = \\frac{T_p}{T_p + F_n}$$\n","Συχνά χρησιμοποιούμε και το ($F_1$) score, το οποίο είναι ο αρμονικός μέσος της ακρίβειας και της ανάκλησης.\n","$$F1 = 2\\frac{P \\times R}{P+R}$$\n","Ιδανικά θέλουμε και υψηλή ακρίβεια και υψηλή ανάκληση, ωστόσο μεταξύ της ακρίβειας και της ανάκλησης υπάρχει γενικά trade-off. Στην οριακή περίπτωση του ταξινομητή που επιστρέφει σταθερά μόνο τη θετική κλάση για παράδειγμα, η ανάκληση θα είναι 1 αλλά η ακρίβεια θα έχει τη μικρότερη δυνατή τιμή της. Γενικά, κατεβάζοντας το κατώφλι της απόφασης του ταξινομητή, αυξάνουμε την ανάκληση και μειώνουμε την ακρίβεια και αντιστρόφως.\n","\n","Στην πράξη και ειδικά σε μη ισορροπημένα datasets χρησιμοποιούμε την ακρίβεια, ανάκληση και το F1 πιο συχνά από την πιστότητα. Επίσης, ανάλογα την εφαρμογή μπορεί να μας ενδιαφέρει περισσότερο ένα συγκεκριμένο metric, πχ η ανάκληση (πχ στη διάγνωση μιας ασθένειας) ή η ακρίβεια (πχ σε μια οικονομική απόφαση)."]},{"cell_type":"code","metadata":{"id":"hbaie_yIN4pk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610724964962,"user_tz":-120,"elapsed":30400,"user":{"displayName":"Chris Chiletzaris","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYqXVJ56Yk9AGghEtEYviJe1INnpbbxzJ9yAYX=s64","userId":"08886392243558337108"}},"outputId":"0df69cfc-1dd7-4be2-e787-f88b6555eac5"},"source":["from sklearn.metrics import precision_recall_fscore_support\n","\n","# εκτυπώνουμε 4 πίνακες, precision, recall, F1 και support. Support είναι ο συνολικός αριθμός προβλέψεων σε κάθε κλάση\n","# το πρώτο στοιχείο του κάθε πίνακα είναι η κλάση setosa, το δεύτερο η versicolor και το τρίτο η virginica\n","print(precision_recall_fscore_support(y_test, pred, average=None), \"\\n\")\n","# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας υπόψη συνολικά (αθροίζοντας εκτός κλάσεων) τα δείγματα (average = micro).\n","print(precision_recall_fscore_support(y_test, pred, average='micro'), \"\\n\")\n","\n","# εκτυπώνουμε το μέσο όρο των precision, recall και F1 θεωρώντας ότι οι κλάσεις έχουν το ίδιο βάρος (average = macro)\n","print(precision_recall_fscore_support(y_test, pred, average='macro'), \"\\n\")\n","\n","# εκτυπώνουμε τa precision, recall και F1 λαμβάνοντας. Με average = weighted κάθε κλάση μετρά στο μέσο όρο ανάλογα με το support της.\n","print(precision_recall_fscore_support(y_test, pred, average='weighted'), \"\\n\")\n","\n","# η classification_report τυπώνει πιο ωραία οπτικά σε string τα αποτελέσματα\n","# πρώτα για κάθε κλάση και μετά με μέσους όρους\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","from sklearn.metrics import classification_report\n","print(classification_report(y_test, pred, target_names=label_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(array([1.        , 1.        , 0.89473684]), array([1.        , 0.91304348, 1.        ]), array([1.        , 0.95454545, 0.94444444]), array([20, 23, 17])) \n","\n","(0.9666666666666667, 0.9666666666666667, 0.9666666666666667, None) \n","\n","(0.9649122807017544, 0.9710144927536232, 0.9663299663299663, None) \n","\n","(0.9701754385964912, 0.9666666666666667, 0.9668350168350168, None) \n","\n","              precision    recall  f1-score   support\n","\n","      setosa       1.00      1.00      1.00        20\n","  versicolor       1.00      0.91      0.95        23\n","   virginica       0.89      1.00      0.94        17\n","\n","    accuracy                           0.97        60\n","   macro avg       0.96      0.97      0.97        60\n","weighted avg       0.97      0.97      0.97        60\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3NgAbLr4ZhSB"},"source":["Documentation από το scikit σχετικά με την αξιολόγηση των μοντέλων:\n","\n","* [sklearn.metrics.precision_recall_fscore_support](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)\n","* [Model evaluation: quantifying the quality of predictions](http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification)\n","* [Multiclass and multilabel classification](http://scikit-learn.org/stable/modules/model_evaluation.html#multiclass-and-multilabel-classification)\n","\n","Σημειώστε ότι όταν η classification report μέχρι το version 0.19 του scikit τυπώνει μόνο ένα μέσο όρο, τον weighted. (σχετική συζήτηση [εδώ](https://stackoverflow.com/questions/23914472/strange-f1-score-result-using-scikit-learn))."]},{"cell_type":"markdown","metadata":{"id":"yD1gabVouubh"},"source":["# Άσκηση: Σύγκριση του Gausian Naive Bayes και του kNN στo Pima Indians Diabetes dataset\n","![1889 Photograph shows half-length portrait of two Pima Indians, facing front, wearing bead necklaces.](https://i.pinimg.com/236x/60/05/76/600576905d4ad5bb1a9c3e3387b397ca--pima-indians-native-american-indians.jpg \"1889 Photograph shows half-length portrait of two Pima Indians, facing front, wearing bead necklaces.\")\n","\n","Διαβάστε το [\"pima-indians-diabetes.data.csv\"](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv) σε ένα numpy array data και ξεχωρίστε features και labels.\n","\n","Για 40% test set: \n","1. υπολογίστε την πρόβλεψη του Gaussian Naive Bayes με τη μέθοδο predict(). \n","2. πάρτε τις προβλέψεις ενός kNN με k=5\n","3. Για έναν ταξινομητή kNN, με 3-fold cross validation και με μετρική 'f1_weighted' υπολογίστε το βέλτιστο k στο train set (maximum k=50). \n","4. εκτυπώστε με την \"classification_report\" τα precision, recall, f1, support για τον NB, τον kNN με k=5 και με το k που έχει προκύψει από cross validation.\n","5. Κάντε 3 runs και αποθηκεύστε σε ένα κελί markdown το average F1 του non optimized και του optimized kNN. Πόσο % έχει βελτιβωθεί η επίδοσή του; \n","\n","hint: Για τη δημιουργία πινάκων σε markdown μπορείτε να χρησιμοποιείτε ένα [markdown table generator](https://www.tablesgenerator.com/markdown_tables)"]},{"cell_type":"code","metadata":{"id":"dCi7CRxTSPKJ"},"source":[""],"execution_count":null,"outputs":[]}]}